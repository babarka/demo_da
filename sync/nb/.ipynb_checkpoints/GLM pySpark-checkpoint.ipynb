{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<dl>\n",
    "<table width=\"250px\">\n",
    "  <tr>\n",
    "    <td width=\"52px\"><img src=\"./kernels/pyspark/logo-64x64.png\" alt=\"H2O R\" style=\"width:48px;height:48px;\"></td>\n",
    "    <td bgcolor=\"#EFEEEC\"><h1><font color=\"#EFEEEC\">__</font>  pySpark</h1></td>\n",
    "  </tr>\n",
    "</table>\n",
    "</dl>\n",
    "**************\n",
    "\n",
    "Apache [Spark](http://spark.apache.org/) is an open source cluster computing framework. Unlike Hadoop's two-stage disk-based MapReduce paradigm, Spark uses multi-stage in-memory primitives. These primatives can be managed in a couple ways, we'll be using YARN. The following script will create an SparkContext, sc, which will then initiate a Spark Application using YARN as the cluster manager. We will use the python Spark API, more commonly known as pySpark.\n",
    "\n",
    "# Invoke Spark-On-YARN Application using Python API (pySpark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "execfile('../init/spark_init.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************\n",
    "# Verify Spark-On-YARN Application is Running:\n",
    "Check the status of your application, named **pySpark**, within the [YARN RUNNING Applications](http://localhost:8088/cluster/apps/RUNNING) List."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************\n",
    "# Create DataFrame Using HiveContext\n",
    "A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in Python, but with richer optimizations. DataFrames can be constructed from a wide array of sources such as: \n",
    "   * **Tables in Hive**\n",
    "   * Structured data files (from local FS or HDFS)\n",
    "   * External databases\n",
    "   * Existing RDDs\n",
    "\n",
    "We will use an existing Hive table, dat. We generated and loaded this data from script; we know it has the following properties:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = \\beta_0 + \\sum_{i=1}^{3} \\beta_i x_i$$\n",
    "$$ \\beta_0: 10 $$\n",
    "$$ \\beta_1:  3 $$\n",
    "$$ \\beta_2: -2 $$\n",
    "$$ \\beta_3: -1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = sqlCtx.sql(\"SELECT y, x1, x2, x3 FROM dat\")\n",
    "dat.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RDD in LabeledPoint Form From DataFrame\n",
    "We ultimately want to evaluate our data using MLlib. In MLlib, labeled training instances are stored using the [LabeledPoint](https://spark.apache.org/docs/1.3.0/api/python/pyspark.mllib.html?highlight=labeledpoint#pyspark.mllib.regression.LabeledPoint) object. So we'll write a parsePoint function that takes as input a raw data point and returns a LabeledPoint and map our records to the labeledPoint form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "\n",
    "def parsePoint(point):\n",
    "    return LabeledPoint(point[0], np.array(point[1:]))\n",
    "\n",
    "dat_lp = dat.map(parsePoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model on RDD in LabeledPoint Form\n",
    "With our data in proper form, we can train our data using [LinearRegressionWithSGD](https://spark.apache.org/docs/1.3.0/api/python/pyspark.mllib.html?highlight=labeledpoint#pyspark.mllib.regression.LinearRegressionWithSGD). There are quite a few parameters available, so be sure to check out [pySpark 1.3.0 Documentation](https://spark.apache.org/docs/1.3.0/api/python/pyspark.mllib.html?highlight=labeledpoint#pyspark.mllib.regression.LinearRegressionWithSGD) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegressionWithSGD.train(data=dat_lp,intercept=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything ran as expected, we should see model weights that estimate the true weights used to generate our dataset:\n",
    "\n",
    "(weights=[3.0,-2.0,-1.0], intercept=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Spark-On-YARN Application:\n",
    "If you were to close out of this python session, the spark context would be destroyed and YARN would kill spark applicaiton. Alternately, you can stop the spark application and continue to work in the python session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Verify Spark-On-YARN Application is Finished:\n",
    "Check the status of your application, named **pySpark**, within the [YARN All Applications](http://localhost:8088/cluster) List."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
